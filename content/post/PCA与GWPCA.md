+++
title = "PCA与GWPCA"  # 文章标题
date = 2020-04-08T23:35:28+08:00  # 自动添加日期信息
draft = false  # 设为false可被编译为HTML，true供本地修改
tags = ["GIS"]  # 文章标签，可设置多个，用逗号隔开。Hugo会自动生成标签的子URL
categories = ["GIS"]
toc = true
preserveTaxonomyNames = true
disablePathToLower = true

+++

> 本文为原创文章，转载注明出处，欢迎关注网站[https://hkvision.cn](https://hkvision.cn)

## 缘起

其实没什么缘起，本人的专业是地理信息系统，结果之前全写的计算机的内容，貌似完全偏题了，这次我来写一下最近做的GWPCA的内容。

## PCA

### 原理

中文名叫主成分分析，做数据分析的人一定对这个不陌生，一般来说做PCA的都是用于数据降维，数据压缩等等。那么为什么这个PCA能够做到数据降维呢？

首先，大家明确一点的是，一堆数据给你了，那么这个数据一定得是有意义的数据，也就是说，我给你n个数据，我希望你能给我n个不同的信息（当然一般不可能），你别给我了n个数据，结果你给我的n个数据是一模一样的，那和给我1个数据没有什么区别。那么用于描述这堆数据的有意义的程度的一个指标——信息熵，就出来了。也就是说你这堆数据到底蕴含了多少信息。

那么一堆数据是没什么意思的，但是这堆数据里面蕴含的信息才是我们需要关注的内容，那么有没有什么办法能够让我们将这堆数据的最精华的部分（也就是蕴含的信息，剔除了冗余的部分）选择出来呢？这个时候我们的主成分分析就能派上用场了。

一个直观的感受是，如果我们这堆数据蕴含了很多信息，那么这堆数据一定有很多不一样的值，毕竟多样化肯定蕴含的信息就多，那么很多不一样的值则意味着我们的数据的方差就会比较大（个人的直观感受）。

那么PCA其实就是，将原先的m维属性，改变其坐标系，换到另外一个坐标系上，用线性代数的话来说就是，给我们原先的m维的特征空间换一个基，当然这个基也得保证相互正交。

换基的过程是这样的：首先在原先的特征空间上选择一个基底，使得所有的数据投影到这个方向上的点的方差最大，然后在保证与第一个基底正交的情况下，选择第二个方向使得方差最大，然后依次类推下去...那么我们就得到了m维特征空间的一组新基，第一个基底也就是我们的第一主成分，然后第二，第三主成分依次...

那么我们可以看到，我们的第一主成分由于数据点投影上去的方差最大，因此包含了最多的信息，后面的依次递减，这样其实就满足了我们的需求——我们把最精华，次精华，次次精华...的部分都选择了出来。

既然我们将最精华的一些部分选择了出来，那么实际上我们可以将那些不怎么精华的部分去掉，也就是减一些列，列减掉了，自然就是给数据进行了降维，并且还不损失太多的信息，这也就是PCA进行数据降维的原理。

### 计算方式

那么PCA如何计算呢？其实也不难，对于数据$X$，首先需要计算协方差矩阵$\sum$
$$
\sum = X^TX
$$


然后将$\sum$进行特征分解
$$
\sum = Q\Lambda Q^{-1}
$$
其中$Q$即为我们的特征向量矩阵，$\Lambda$则是由特征值构成的矩阵，此时我们将特征值进行排序，并将特征向量矩阵也进行相应的排序，那么我们就已经做好了PCA了。其中特征向量矩阵就是我们投影的参数，即
$$
S = XQ
$$
其中S即为投影后的矩阵，也就是我们的第一，第二...主成分。

### 预处理

在PCA之前我们需要将数据先进行预处理，预处理的方式即
$$
A' = A-E(A)
\\
X = \frac{A'}{\sigma(A')}
$$
其中$\sigma{A'}$是$A'$的标准差。这样做预处理的目的是进行标准化，一般来说减去均值即可，但是由于我们的计算涉及到协方差，因此需要进行标准化而不仅仅是归一化。

### SVD

SVD即奇异值分解，其实可以理解成非方阵的特征分解。SVD分解是PCA的一种解法，至于原因，则是因为SVD为了解决行列不相同的问题，将原始数据乘了其转置，然后对此进行特征分解。也就是说原本是
$$
X = U \Sigma V^T
$$
结果实际上在解算的过程中是对$X^TX$进行了特征分解，也就是说，其本身和PCA的解算有相似之处。实际上我们对$X^TX$进行特征分解就是要求右奇异矩阵$V$，而某些SVD算法本身是不需要进行特征分解的，也就是说，我们不需要进行特征分解就能得到PCA的结果，这在样本量很大的时候非常有效。

## GWPCA

GWPCA就是地理加权PCA，也就是对每个样本赋予了不同的权重。做GIS的人都知道地理学第一定律：空间上临近的样本，其属性也应相近。也就是说，如果我和你很近，那么我们俩应该差不多，房子旁边大概率还是房子，不会有哪个人孤独的在草原上弄个别墅，水周边大概率还是水等等等等。或者是，如果你家的房子，价格2w，那么你邻居家的房子大概也是2w。那么地理加权的意义就在于，考虑其空间属性，对于某一个地理尺度上的局部地区的协方差结构，进行PCA。也就是说，对于每个样本点，我们均进行PCA的解算，样本点就是我周边一个范围内的点，并且我对其进行地理加权，那么我探索的，就是局部的主成分。

### 解算

对于每个样本点，需要设定一个带宽范围bw和一个核函数，这两个参数决定了随着距离的增加，权重的衰减情况，那么在计算得到了每个点所应有的权重后，我们来计算地理加权的协方差矩阵
$$
\Sigma = X^TWX
$$
之后就和PCA的解算过程是一致的，当然预处理的部分也不能少，对于GWPCA来说，其预处理过程是这样的。
$$
X = A_i - \frac{\Sigma A_iw_i}{\Sigma wi}
$$
老实说，我也不知道为什么要这样，是从[`GWmodel`]([https://CRAN.R-project.org/package=GWmodel](https://cran.r-project.org/package=GWmodel) )包里面的源码得来的。

## 后记

之后我会写一下如何进行带宽优选（等我自己先研究透彻:smile:）



## 参考

- [奇异值分解](https://zhuanlan.zhihu.com/p/29846048)
- Harris P, Brunsdon C, Charlton M (2011) Geographically weighted principal components analysis. International Journal of Geographical Information Science 25:1717-1736

